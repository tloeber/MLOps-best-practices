{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating data science and ML engineering: Designing a collaboration and handoff process between data scientists and ML engineers\n",
    "\n",
    "- ## Challenge: Different needs of data science vs ML engineering\n",
    "\n",
    "- In both we want agility – but this is achieved in different ways:\n",
    "  - In data science, we achieve agility through the explorative and iterative notebook workflow.\n",
    "  - By contrast, for a production software system to stay agile, we require clean code, type safety, moving away from notebooks, and loose coupling between components (e.g., stable interfaces).\n",
    "- -> **Corollary: the former does not automatically scale into the latter**\n",
    "  - What leads to agility in model training leads to a lack of agility in model deployment and maintenance.\n",
    "  - Different best practices / quality standards for each\n",
    "  - Need to find a good process for collaboration and handoff\n",
    "\n",
    "- ## Side note on terminology: What about ML scientists?\n",
    "\n",
    "I don't like arguing about terminology, but unfortunately we have to get rid of source of confusion first...\n",
    "\n",
    "- “ML scientist” vs “ML engineer”\n",
    "  - Confusingly, ML scientists  are often called \"ML engineers\".\n",
    "  - But “engineering” refers to an approach to building reliable, production-grade systems.\n",
    "- “ML scientist” vs “data scientist”\n",
    "  - Basically, a ML scientist is a data scientist working with models requiring a higher level of advanced ML expertise.\n",
    "  - Like data scientists, their emphasis is on getting a ML model to work, rather than ongoing maintainability.\n",
    "  - E.g., they may spend time on performance optimization of compute bottlenecks, but are less concerned with the readability and maintainability of their code, or how easy it is to run it on another machine.\n",
    "  - -> For our purposes, we can subsume both under the same category. I will use the term “data scientist” to refer to both.\n",
    "\n",
    "## Conflicting best practices in data science and ML\n",
    "\n",
    "- Starting point: Need to acknowledge this dilemma:\n",
    "  - unique *needs* of both sides (as we just discussed)\n",
    "  - unique *talents* of both sides (division of labor)\n",
    "- Next step: Codify best practices / quality standards separately for each side.\n",
    "- ~~Keep in mind: How to structure incentives~~\n",
    "\n",
    "### Data science vs engineering\n",
    "\n",
    "- How data science *differs* from engineering:\n",
    "  - explorative and iterative -> notebook workflow\n",
    "  - Higher importance of domain expertise\n",
    "    - Do these data make sense?\n",
    "    - What way of computing this feature makes the most sense from a domain perspective?\n",
    "- How data science *supplements* engineering:\n",
    "  - Exploration of the data by someone with domain expertise can:\n",
    "    - surface problems\n",
    "    - create new ideas\n",
    "  - But any changes resulting from these insights should be addressed:\n",
    "    - using *production-grade* fixes rather than hacks (implemented by engineers, based on insights from data scientists)\n",
    "    - at the *source* (rather than each data scientist reinventing the wheel by performing the same data cleaning downstream)\n",
    "\n",
    "## Why not *all* engineering best practices apply to data science\n",
    "\n",
    "- Applying a specific engineering best practice in data science can be:\n",
    "  - Counterproductive: specific needs of the data science process\n",
    "  - Productive - but there may be hurdles to adoption:\n",
    "    - adoption cost (can we lower it sufficiently?)\n",
    "    - not well-known enough (educate!)\n",
    "    - incentives encourage short-term focus (same problem as in software engineering)\n",
    "  - Neutral to data scientist productivity (We may as well adopt them early on to make handoff easier)\n",
    "\n",
    "### Why *some* engineering best practices may be *counterproductive* in data science\n",
    "\n",
    "Some engineering practices are too constraining (their cost is not offset by large enough concomitant benefit)\n",
    "\n",
    "- Code is short-lived -> maintainability is less important\n",
    "- Lesser need to foresee possible problems; instead, take a close look at actual data, and react to problems as we observe them.\n",
    "- Interactive data science workflow provides different ways of addressing certain problems. E.g.:\n",
    "  - Knowing the data schema beforehand is less important, because we can just take a look at the data and fix any problems we observe.\n",
    "  - In a notebook, type hints are less important for readability because we can interactively inspect what variable looks like if we're not sure.\n",
    "\n",
    "### Why *some* engineering best practices *are relevant* to data science\n",
    "\n",
    "- Even though data science code is more short-lived, and long-term maintainability is thus less important, changeability is still important due to the iterative nature of the explorative workflow.\n",
    "\n",
    "## Codifying best practices\n",
    "\n",
    "### ~~Shared best practices~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Comments:\n",
    "\n",
    "-   Open questions\n",
    "    - Is organizing by must have versus should have the best idea?\n",
    "    - Where do we draw the line between must versus should have? \n",
    "    - How do we deal with differences in degree rather than differences in kind? For example, a basic amount of clean code is clearly a must-have for ML engineering, but some aspects of it may only be should- or even would-like-to haves.\n",
    "\n",
    "-  Alternatives:\n",
    "    - ~~by maturity level~~ \n",
    "      - Corresponds to must/should/would like to haves: Low maturity corresponds to not having any; medium maturity corresponds to having must-haves, high corresponds to additionally satisfying should-haves, and highest corresponds to also satisfying would-like-to-haves.\n",
    "    - Should we specify which are the minimum requirements once we enter production? Or could just use color-coding!\n",
    "\n",
    "-  Note that these also may differ by:\n",
    "    - use case (e.g., inherent complexity of data transformations and ML modeling)\n",
    "    - POC versus production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### *Data science* best practices\n",
    "\n",
    "#### Must-haves\n",
    "- Background infra / team investment\n",
    "  - Quick & easy dev setup\n",
    "    - Align incentives: The recommended way should be the easy way. \n",
    "      - Don't just write up the required setup steps in a readme – instead, put them in a script whenever possible! [Makefiles are ideals for this](./_details/why_makefiles.md).\n",
    "  - Availability of tooling to:\n",
    "    - manage the complexity that arises out of the *iterative* workflow \n",
    "      - In particular, experiment management & model registry\n",
    "    - deal with *data versioning*\n",
    "      - For basic use cases, Data Version Control may be a good enough starting point.\n",
    "      - If dealing with data with a time-series component, the ability to time-travel (e.g., retrieve a snapshot of how the data looked like at a given date) becomes essential in order to avoid information leakage, which is a common source of training-serving skew.\n",
    "        - This functionality could either come from either:\n",
    "          - the underlying data infrastructure (e.g. it is available in Snowflake, DeltaLake, or Apache Iceberg tables); or\n",
    "          - a dedicated feature store.\n",
    "- Team processes:\n",
    "  - *Common strategy* for how to *version*:\n",
    "    - models/experiments\n",
    "      - Purpose: Avoid being overwhelmed by the great number of experiments\n",
    "      - Goals:\n",
    "        - Reproducibility -> Needs to log all relevant parameters (including data versions)\n",
    "        - Low overhead -> Ideally, all experiments are automatically logged, without requiring custom code, etc.: \n",
    "    - data\n",
    "    - code\n",
    "  - Environment and dependency management: Environment should be *easily reproducible* for others. See more detailed notes [here](../coding/python/readme.md).\n",
    "    - Package manager: Use pip over conda, if at all possible\n",
    "    - For short-lived notebooks, it is usually sufficient to track *either* abstract or concrete dependencies. \n",
    "    - However, if dependency conflicts become common, it is time to track both (e.g., using pipenv).\n",
    "    - It should be clear which (minor) version of python  to use to re-create an environment. Unfortunately, there is no way to encode the required python version in a requirements.txt file. So the main choices are:\n",
    "      - If we want to stick with pip-installing a requirements.txt, the best we can do is probably to define all environment-related commands in a Makefile that explicitly hard-codes the minor version of Python.\n",
    "      - Alternatively, this challenge alone can be a good reason to already learn how to use more sophisticated tools such as pipenv instead (which also bring additional benefits).\n",
    "    - If run in a managed notebook environment, it should be clear which instance size and kernel is required to run the notebook.\n",
    "    - Make sure to use reasonably up-to-date versions. E.g., don't reuse old environment for new project; periodically update dependencies, etc.\n",
    "  - Align with ML engineers on choices that have a downstream impact (deployability, maintainability, etc), to avoid the risk of going down an inferior path.\n",
    "    - Double-check that environment creation process is sufficiently reproducible for production deployment\n",
    "    - For important packages, align on which version can be used\n",
    "      - e.g., don't use any version of Python that is at or near its end of life. Usually, a \"middle-aged\" minor version of Python is the best choice, because it often takes surprisingly long until the newest version is supported everywhere.\n",
    "    - Before creating new features or training a new model, align on how the different options of doing so make it easier or harder to productionize it.\n",
    "      - This is especially important if re-training would be prohibitively expensive.\n",
    "      - At the very latest, this should be done before producing anything (code, data, models) that may be used in production. For example, this excludes running experiments on a small subset of the data, where the main purpose is to get the code to work. \n",
    "      - However, the smartest point to do this is usually even earlier than that, namely *before* investing a considerable amount of time into trying out something new.\n",
    "      - E.g., if Sagemaker Pipelines is used for deployment, it is easiest and safest to already use a Sagemaker Processor for preprocessing and feature engineering, and to use [an estimator from Sagemaker](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html) for model training. \n",
    "\n",
    "- individual\n",
    "  - Follow a iterative and interactive workflow\n",
    "  - ...but *clean up* code before handing off to others (whether fellow data scientists or ML engineers)\n",
    "    - Clean up duplication.\n",
    "      - Generally, the place to store the results from experiments with different parameters is an experiment management solution (e.g., MLFlow) – not copy-pasted code cells in the same notebook.\n",
    "    - Notebook output cells should reflect a run from top to bottom. \n",
    "      - This is not easy if notebook includes expensive calculations, which makes it impractical to simply restart kernel and then re-run whole notebook before committing. \n",
    "      - There are some tools that are supposed to help others, but I have not looked into them.\n",
    "      - [The best solution may be to simply limit the use of notebooks beyond initial experimentation phase](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68282.html).\n",
    "  - Learn to *use* the essential tooling discussed above\n",
    "\n",
    "#### Should-haves\n",
    "\n",
    "- Clean code\n",
    "- Invest time to find good data-science tools for job\n",
    "- Use engineering tools determined to be worth the investment\n",
    "  - engineering team should help with tool evaluation, recommendation, and set up\n",
    "  - Examples: Use type hints, depending on maturity\n",
    "  - leverage the power of a proper IDE (rather than notebook in browser)\n",
    "\n",
    "#### Would-like-to-haves\n",
    "\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "- Trusted test suite (automated unit and integration/acceptance tests); automated data validation, static analysis\n",
    "\n",
    "#### Does-not-need-to-haves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *ML Engineering* best practices\n",
    "\n",
    "### Must have\n",
    "\n",
    "- Clean code\n",
    "  - Rationale:\n",
    "    - Code is read much more of than written -> it should be optimized for readability.\n",
    "    - Reduces bugs.\n",
    "    - Increasing Agility, because it makes code easier to change.\n",
    "    - Overall, reduces maintenance cost (which is majority of the cost of a typical software project)\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "  - Rationale:\n",
    "    - Greatly reduces complexity, thereby ensuring code stays maintainable (reduces cost and risk, while increasing speed of feature implementation)\n",
    "\n",
    "- Trusted, automated test suite\n",
    "  - Components:\n",
    "    - unit tests\n",
    "    - integration/acceptance tests\n",
    "    - data validation\n",
    "    - static analysis\n",
    "  - Rationale:\n",
    "    - Increases quality (reduces errors, outages, etc.)\n",
    "    - Decreases costs, since the cost of bugs rises the later in the SDLC they are discovered (\"shifting left\")\n",
    "    - Indirect benefit: A trusted test suite makes sure that engineers are not dominated by their own creation\n",
    "- Type safety:\n",
    "  - code: use type hints, and force in CI pipeline\n",
    "  - data: use explicit data schemas\n",
    "- Observability:\n",
    "  - code\n",
    "    - structure & centralized logging\n",
    "    - monitoring and alerting\n",
    "    - distributed tracing if using micro service architecture\n",
    "  - model performance\n",
    "    - comparison between different models\n",
    "    - comparison of same model over time (model drift?)\n",
    "    - performance for different subsets of the population/bias (if substantial, does this vary by model?).\n",
    "  - data\n",
    "    - data quality\n",
    "- DevOps\n",
    "  - Infrastructure-as-code\n",
    "  - CI/CD\n",
    "- Data lineage\n",
    "- Invest time to find good tools for the job\n",
    "  - Avoid reinventing the wheel -> Leverage managed services wherever possible (unless “unfair” pricing)\n",
    "  - leverage power of IDE (rather than notebook in browser)\n",
    "\n",
    "### Should have\n",
    "\n",
    "- Profile code and optimize performance bottlenecks\n",
    "  - (Why we do not consider this a must-have: Engineers' time is very expensive, and so is delaying features or accumulating technical debt, so unfortunately performance optimization has sometimes to be sacrificed for these even more important goals.\n",
    "- Periodically reevaluate if there are better tools for the job\n",
    "  - e.g., Pandas alternatives (such as Polars)\n",
    "  - e.g., Spark vs Presto\n",
    "  - e.g., End-to-end (Sagemaker ) versus best-off-breed MLOps tools\n",
    "\n",
    "### Would-like to have\n",
    "\n",
    "### (Does not need to have:)\n",
    "\n",
    "- Best practices for data science:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "See other slide deck for details.\n",
    "\n",
    "- Following these standards should be part of the definition of done. (Avoid the \"mini-waterfall\", e.g. putting tests in a separate story and refactoring in yet another story.)\n",
    "- Align incentives: Hold teams and organizational unit accountable\n",
    "\n",
    "## How engineering complements data science\n",
    "\n",
    "- There are plenty of cases where engineering principles can benefit data scientists, but:\n",
    "  - there is a substantial adoption cost:\n",
    "    - if you can bring adoption cost down, it may make sense to include some engineering best practices as data science best practices\n",
    "  - There is a knowledge gap\n",
    "  - the adoption threshold seems too high\n",
    "\n",
    "## Collaboration between data science and ML\n",
    "\n",
    "### Recommendation: Create cross-functional teams of data scientists and ML engineers\n",
    "\n",
    "- Vertical slicing in Agile (cross-functional teams / slice by value)\n",
    "  - Data pipeline, model training, deployment\n",
    "- Handoff between data science and ML is challenging (even with the best possible process)\n",
    "  - Requires collaboration\n",
    "  - Collaboration is much easier within a team (rather than between teams)\n",
    "  - Remember the lessons from DevOps (don’t just throw models over the wall) and microservices.\n",
    "- Collaboration early in the ML lifecycle reduces the risk of introduces bugs during productionization\n",
    "  - e.g., if ML engineers refactor data transformation code before it is used in model training pipeline, we can make sure that we apply exact same transformations in training and inference, thereby eliminating a common source of training-serving skew\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- Organizational inertia: If data scientists are located on the business side, it’s tempting to just add a new MLOps team within IT/engineering\n",
    "- Cultural differences between data scientists and engineering\n",
    "  - It’s tempting to separate both sides into different teams.\n",
    "  - But: This perpetuates these differences and makes collaboration hard.\n",
    "  - Remember again the lessons from DevOps!\n",
    "\n",
    "## What exactly needs to be handed off?\n",
    "\n",
    "- We need to productionize:\n",
    "  - Code\n",
    "  - Models\n",
    "  - Data (transformations)\n",
    "\n",
    "### Handoff, 1): Productionizing models\n",
    "\n",
    "- Easiest (compared to code and data)\n",
    "- Model registry can serve as a convenient hand-off point\n",
    "- Potential problems:\n",
    "  - Changes to model interface (incl. schema of input data)\n",
    "  - Dependency management\n",
    "- Solution: Formal contracts that are automatically enforceable\n",
    "  - Standard process for environment creation\n",
    "  - Better yet: Use containers\n",
    "  - Define model interface and data schemas in shared libraries\n",
    "    - Class interface can be cheaply enforced through static analysis (mypy) in CICD pipeline (and IDE plugin)\n",
    "    - Data schema checks sometimes require running the process for validation, so it’s more expensive. Make this part of acceptance tests.\n",
    "      - E.g., pandera\n",
    "    - Note that steps also bring huge side-benefits in terms of documentation and system understandability\n",
    "\n",
    "### Handoff, 2: Productionizing code\n",
    "\n",
    "- Harder than productionizing models.\n",
    "- Question: To what extent can you expect data scientists to follow software engineering best practices?\n",
    "  - Remember: Different needs and talents -> It's not realistic to expect data scientists to become engineers.\n",
    "  - That said, there is also an overlap in best practices -> In some areas, data scientists can become more productive by leveraging insights from software engineering\n",
    "    - E.g., power of IDE, type hints, basic understanding of what makes code changeable (because even though data science code is typically short-live, changeability is still highly important due to the iterative nature of the workflow).\n",
    "- -> Need to find the right balance, taking into consideration:\n",
    "- Which parts of software engineering best practices would also benefit data scientists\n",
    "- How much more costly it is to add something after versus before the handoff\n",
    "- How hard it is / how costly it is (in terms of one-time investment) for data scientists to learn required skills\n",
    "- -> The right point on this trade-off varies:\n",
    "  - over time\n",
    "    - can't expect data scientists to learn a bunch of new skills at once\n",
    "    - easier to get buy-in from data scientist if we start with quick wins that help them see the benefits for themselves\n",
    "  - between companies (and even between teams)\n",
    "    - Ability to attract top talent (pay premium, corporate brand, etc.)\n",
    "    - Different specialties within data science require different level of engineering knowledge\n",
    "\n",
    "- -> Challenge: Balance quality control with ability of data science team to self-serve\n",
    "- Suggested steps:\n",
    "\n",
    "#### 1) Make it as easy as possible for data scientists to follow software development best practices\n",
    "\n",
    "- e.g., provide templates for recommended IDE configuration, etc.\n",
    "- Ideally, laptops should already come preconfigured with recommended settings for each job function.\n",
    "- This is an example of how to properly structure incentives!\n",
    "- ML Engineers should make themselves available to help (and coach) with things in their area of expertise. And vice versa.\n",
    "\n",
    "#### 2) Leverage automatic code improvement tools\n",
    "\n",
    "- -> Automate what can be automated\n",
    "- linting and formatting (AutoPEP-8, Black, YAPF)\n",
    "- adding type hints (e.g., monkeytype)\n",
    "\n",
    "#### 3) Manual re-factoring by ML engineers\n",
    "\n",
    "- Performance: e.g., convert panda map/apply to vectorized operation of possible;\n",
    "- Reliability (e.g., Type-safety: add type hints, add data schemas (especially if read from external source))\n",
    "- Maintainability: Readability,\n",
    "- Testability: extract functions\n",
    "\n",
    "#### 4) Find production-ready solution for any hacks\n",
    "\n",
    "- E.g., unofficial data sources need to be productionized\n",
    "\n",
    "### Handoff, 3: Productionizing data\n",
    "\n",
    "- How do you productionize data?\n",
    "  - Outside the scope: data governance strategy, etc.\n",
    "    - Should be defined at the organizational level\n",
    "    - If non-existent or not sufficient, ML engineering may want to be part of this discussion to communicate the needs of ML.\n",
    "  - In scope:\n",
    "  - Bring unofficial data sources into the organization's official data governance realm\n",
    "    - E.g., Excel files\n",
    "  - Productionize data transform code\n",
    "    - Reduces to the previous problem of productionizing code\n",
    "  - Productionize features feature store\n",
    "    - Store transformations in feature store\n",
    "\n",
    "- Additional challenges: Productionizing data requires wider support from leadership due to upstream dependencies\n",
    "  - Assign data owners who are domain experts\n",
    "  - Collaborate with data owners to fix any data problems that data scientists discovered at the source\n",
    "  - Ideally, the general data engineering (“silver tables”) is handled by dedicated teams/data engineers.\n",
    "  - In the short term, ML engineers may have to lend a hand in order to show the value of this model (and because they have an interest in it).\n",
    "  - In the long term, ML engineers should only be productionizing data transformations that are related to feature engineering or are very specific to their use case (\"gold tables\")\n",
    "\n",
    "- Productionizing data is the hardest problem of the 3\n",
    "\n",
    "#### Goals\n",
    "\n",
    "- Ensure data quality\n",
    "  - Trustworthiness\n",
    "  - Data problems should be addressed at the source, by someone familiar with the domain -> data users should be able to rely on quality\n",
    "- efficiency: Don't make people reinvent the wheel\n",
    "  - Data problems should be fixed once at the source, not by every user\n",
    "    - Quicker for a domain expert to validate data quality and fix potential problems\n",
    "    - Domain expert can do a better job at detecting problems and making decisions on how to fix any issues found\n",
    "    - Has to be done only once rather than multiple times\n",
    "  - It should be easy to share relevant data and features, and to discover them in the first place easy\n",
    "- Standardize feature calculations\n",
    "  - -> comparability across use cases\n",
    "- data lineage / provenance\n",
    "- Potentially: Low latency for real-time inference\n",
    "\n",
    "## How to structure Handoff?\n",
    "\n",
    "- Need to find a handoff process that works for both sides\n",
    "  - Acknowledge different needs of both sides\n",
    "  - Danger: Compromises will be required on both sides; be careful the outcomes are not driven by the prevailing balance of power\n",
    "- Especially if handoff is between teams: Define stable, formal contract. Enforce automatically if possible.\n",
    "  - e.g., data schemas, API schema, gives interfaces/data classes defined in shared libraries, Gherkin scenarios. Enforced in CI/CD pipeline.\n",
    "  - …because this decouples teams from each other, thus reducing blockers and communication inefficiencies\n",
    "  - Requires an engineering mindset\n",
    "  - Still beneficial  - though less important - within cross-functional team\n",
    "- How to make handoff work?\n",
    "  - Quick feedback loop\n",
    "    - In the best case, static analysis plug-ins in the IDE tells us when we are violating an interface right as we are writing the code.\n",
    "  - Align incentives\n",
    "    - If one side breaks the contract, it should be clear which side did so, and accordingly has to fix it.\n",
    "      - E.g., test is run automatically in pipeline and blocks merging.\n",
    "    - We ideally want to avoid bothering the other side with alerts they did not cause.\n",
    "\n",
    "# Maturity levels\n",
    "\n",
    "- Level 0: Only ad-hoc communication between teams of what and how to hand off.\n",
    "  - Characteristics: handoff often doesn't go smoothly:\n",
    "    - \"it works on my machine problem\" (e.g., because there is no easy way to re-create an identical virtual environment\n",
    "    - ML team is not able to satisfy frequent request for changes (e.g., adding an additional feature)\n",
    "- Level 1: Formal handoff process that divides responsibilities\n",
    "- Level 2: Formal handoff process + collaboration throughout the ML lifecycle to minimize handoff\n",
    "  - \"shifting left\": Where needs of engineering are not opposed to needs of data science, it's cheaper to introduce engineering requirements earlier in the ML lifecycle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presentations-X1dq4RR9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
