{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Collaboration* between data science and ML\n",
    "\n",
    "## Recommendation: Create cross-functional teams of data scientists and ML engineers\n",
    "\n",
    "- Vertical slicing in Agile (cross-functional teams / slice by value)\n",
    "  - Data pipeline, model training, deployment\n",
    "- Handoff between data science and ML is challenging (even with the best possible process)\n",
    "  - Requires collaboration\n",
    "  - Collaboration is much easier within a team (rather than between teams)\n",
    "  - Remember the lessons from DevOps (don’t just throw models over the wall) and microservices.\n",
    "- Collaboration early in the ML lifecycle reduces the risk of introduces bugs during productionization\n",
    "  - e.g., if ML engineers refactor data transformation code before it is used in model training pipeline, we can make sure that we apply exact same transformations in training and inference, thereby eliminating a common source of training-serving skew\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- Organizational inertia: If data scientists are located on the business side, it’s tempting to just add a new MLOps team within IT/engineering\n",
    "- Cultural differences between data scientists and engineering\n",
    "  - It’s tempting to separate both sides into different teams.\n",
    "  - But: This perpetuates these differences and makes collaboration hard.\n",
    "  - Remember again the lessons from DevOps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What exactly needs to be *handed off*?\n",
    "\n",
    "- We need to productionize:\n",
    "  - Code\n",
    "  - Models\n",
    "  - Data (transformations)\n",
    "\n",
    "## Handoff, 1): Productionizing models\n",
    "\n",
    "- Easiest (compared to code and data)\n",
    "- Model registry can serve as a convenient hand-off point\n",
    "- Potential problems:\n",
    "  - Changes to model interface (incl. schema of input data)\n",
    "  - Dependency management\n",
    "- Solution: Formal contracts that are automatically enforceable\n",
    "  - Standard process for environment creation\n",
    "  - Better yet: Use containers\n",
    "  - Define model interface and data schemas in shared libraries\n",
    "    - Class interface can be cheaply enforced through static analysis (mypy) in CICD pipeline (and IDE plugin)\n",
    "    - Data schema checks sometimes require running the process for validation, so it’s more expensive. Make this part of acceptance tests.\n",
    "      - E.g., pandera\n",
    "    - Note that steps also bring huge side-benefits in terms of documentation and system understandability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handoff, 2: Productionizing code\n",
    "\n",
    "- Harder than productionizing models.\n",
    "- Question: To what extent can you expect data scientists to follow software engineering best practices?\n",
    "  - Remember: Different needs and talents -> It's not realistic to expect data scientists to become engineers.\n",
    "  - That said, there is also an overlap in best practices -> In some areas, data scientists can become more productive by leveraging insights from software engineering\n",
    "    - E.g., power of IDE, type hints, basic understanding of what makes code changeable (because even though data science code is typically short-live, changeability is still highly important due to the iterative nature of the workflow).\n",
    "- -> Need to find the right balance, taking into consideration:\n",
    "- Which parts of software engineering best practices would also benefit data scientists\n",
    "- How much more costly it is to add something after versus before the handoff\n",
    "- How hard it is / how costly it is (in terms of one-time investment) for data scientists to learn required skills\n",
    "- -> The right point on this trade-off varies:\n",
    "  - over time\n",
    "    - can't expect data scientists to learn a bunch of new skills at once\n",
    "    - easier to get buy-in from data scientist if we start with quick wins that help them see the benefits for themselves\n",
    "  - between companies (and even between teams)\n",
    "    - Ability to attract top talent (pay premium, corporate brand, etc.)\n",
    "    - Different specialties within data science require different level of engineering knowledge\n",
    "\n",
    "- -> Challenge: Balance quality control with ability of data science team to self-serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested steps:\n",
    "\n",
    "#### 1) Make it as easy as possible for data scientists to follow software development best practices\n",
    "\n",
    "- e.g., provide templates for recommended IDE configuration, etc.\n",
    "- Ideally, laptops should already come preconfigured with recommended settings for each job function.\n",
    "- This is an example of how to properly structure incentives!\n",
    "- ML Engineers should make themselves available to help (and coach) with things in their area of expertise. And vice versa.\n",
    "\n",
    "#### 2) Leverage automatic code improvement tools\n",
    "\n",
    "- -> Automate what can be automated\n",
    "- linting and formatting (AutoPEP-8, Black, YAPF)\n",
    "- adding type hints (e.g., monkeytype)\n",
    "\n",
    "#### 3) Manual re-factoring by ML engineers\n",
    "\n",
    "- Performance: e.g., convert panda map/apply to vectorized operation of possible;\n",
    "- Reliability (e.g., Type-safety: add type hints, add data schemas (especially if read from external source))\n",
    "- Maintainability: Readability,\n",
    "- Testability: extract functions\n",
    "\n",
    "#### 4) Find production-ready solution for any hacks\n",
    "\n",
    "- E.g., unofficial data sources need to be productionized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Handoff, 3: Productionizing data\n",
    "\n",
    "- How do you productionize data?\n",
    "  - Outside the scope: data governance strategy, etc.\n",
    "    - Should be defined at the organizational level\n",
    "    - If non-existent or not sufficient, ML engineering may want to be part of this discussion to communicate the needs of ML.\n",
    "  - In scope:\n",
    "  - Bring unofficial data sources into the organization's official data governance realm\n",
    "    - E.g., Excel files\n",
    "  - Productionize data transform code\n",
    "    - Reduces to the previous problem of productionizing code\n",
    "  - Productionize features feature store\n",
    "    - Store transformations in feature store\n",
    "\n",
    "- Additional challenges: Productionizing data requires wider support from leadership due to upstream dependencies\n",
    "  - Assign data owners who are domain experts\n",
    "  - Collaborate with data owners to fix any data problems that data scientists discovered at the source\n",
    "  - Ideally, the general data engineering (“silver tables”) is handled by dedicated teams/data engineers.\n",
    "  - In the short term, ML engineers may have to lend a hand in order to show the value of this model (and because they have an interest in it).\n",
    "  - In the long term, ML engineers should only be productionizing data transformations that are related to feature engineering or are very specific to their use case (\"gold tables\")\n",
    "\n",
    "- Productionizing data is harder than productionizing code and models!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "- Ensure data quality\n",
    "  - Trustworthiness\n",
    "  - Data problems should be addressed at the source, by someone familiar with the domain -> data users should be able to rely on quality\n",
    "- efficiency: Don't make people reinvent the wheel\n",
    "  - Data problems should be fixed once at the source, not by every user\n",
    "    - Quicker for a domain expert to validate data quality and fix potential problems\n",
    "    - Domain expert can do a better job at detecting problems and making decisions on how to fix any issues found\n",
    "    - Has to be done only once rather than multiple times\n",
    "  - It should be easy to share relevant data and features, and to discover them in the first place easy\n",
    "- Standardize feature calculations\n",
    "  - -> comparability across use cases\n",
    "- data lineage / provenance\n",
    "- Potentially: Low latency for real-time inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to structure Handoff?\n",
    "- Need to find a handoff process that works for both sides\n",
    "  - Acknowledge different needs of both sides\n",
    "  - Danger: Compromises will be required on both sides; be careful the outcomes are not driven by the prevailing balance of power\n",
    "- Especially if handoff is between teams: Define stable, formal contract. Enforce automatically if possible.\n",
    "  - e.g., data schemas, API schema, gives interfaces/data classes defined in shared libraries, Gherkin scenarios. Enforced in CI/CD pipeline.\n",
    "  - …because this decouples teams from each other, thus reducing blockers and communication inefficiencies\n",
    "  - Requires an engineering mindset\n",
    "  - Still beneficial  - though less important - within cross-functional team\n",
    "- How to make handoff work?\n",
    "  - Quick feedback loop\n",
    "    - In the best case, static analysis plug-ins in the IDE tells us when we are violating an interface right as we are writing the code.\n",
    "  - Align incentives\n",
    "    - If one side breaks the contract, it should be clear which side did so, and accordingly has to fix it.\n",
    "      - E.g., test is run automatically in pipeline and blocks merging.\n",
    "    - We ideally want to avoid bothering the other side with alerts they did not cause.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "See [other slide](../../presentations/4%20-%20Overcoming%20organizational%20obstacles%20to%20robust%20software,%20data,%20and%20ML%20systems.pdf) deck for details.\n",
    "\n",
    "- Following these standards should be part of the definition of done. (Avoid the \"mini-waterfall\", e.g. putting tests in a separate story and refactoring in yet another story.)\n",
    "- Align incentives: Hold teams and organizational unit accountable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maturity levels\n",
    "\n",
    "- Level 0: Only ad-hoc communication between teams of what and how to hand off.\n",
    "  - Characteristics: handoff often doesn't go smoothly:\n",
    "    - \"it works on my machine problem\" (e.g., because there is no easy way to re-create an identical virtual environment\n",
    "    - ML team is not able to satisfy frequent request for changes (e.g., adding an additional feature)\n",
    "- Level 1: Formal handoff process that divides responsibilities\n",
    "- Level 2: Formal handoff process + collaboration throughout the ML lifecycle to minimize handoff\n",
    "  - \"shifting left\": Where needs of engineering are not opposed to needs of data science, it's cheaper to introduce engineering requirements earlier in the ML lifecycle\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
