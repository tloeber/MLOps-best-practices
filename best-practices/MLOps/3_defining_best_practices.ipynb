{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining best practices\n",
    "## ~~Shared best practices~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "-   Open questions\n",
    "    - Is organizing by must have versus should have the best idea?\n",
    "    - Where do we draw the line between must versus should have? \n",
    "    - How do we deal with differences in degree rather than differences in kind? For example, a basic amount of clean code is clearly a must-have for ML engineering, but some aspects of it may only be should- or even would-like-to haves.\n",
    "\n",
    "-  Alternatives:\n",
    "    - ~~by maturity level~~ \n",
    "      - Corresponds to must/should/would like to haves: Low maturity corresponds to not having any; medium maturity corresponds to having must-haves, high corresponds to additionally satisfying should-haves, and highest corresponds to also satisfying would-like-to-haves.\n",
    "    - Should we specify which are the minimum requirements once we enter production? Or could just use color-coding!\n",
    "\n",
    "-  Note that these may also differ by:\n",
    "    - use case (e.g., inherent complexity of data transformations and ML modeling)\n",
    "    - POC versus production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### *Data science* best practices\n",
    "\n",
    "#### Must-haves\n",
    "- Background infra / team investment ~~/ Environment / Ecosystem~~\n",
    "  - Quick & easy dev setup\n",
    "    - Align incentives: The recommended way should be the easy way. \n",
    "      - Don't just write up the required setup steps in a readme – instead, put them in a script whenever possible! [Makefiles are ideals for this](./_details/why_makefiles.md).\n",
    "  - Availability of tooling to:\n",
    "    - manage the complexity that arises out of the *iterative* workflow \n",
    "      - In particular, a solution for experiment management (e.g., MLFlow). A model registry also falls into this category, as it serves to mark which of the many trained models have passed a quality threshold that makes them fit for deployment.\n",
    "    - deal with *data versioning*\n",
    "      - For basic use cases, Data Version Control may be a good enough starting point.\n",
    "      - If the data has an important time-series component, the ability to time-travel (e.g., retrieve a snapshot of how the data looked like at a given point in time) becomes essential in order to avoid information leakage, which is a common source of training-serving skew.\n",
    "        - This functionality could either come from either:\n",
    "          - the underlying data infrastructure (e.g. it is available in Snowflake, DeltaLake, or Apache Iceberg tables); or\n",
    "          - a dedicated feature store.\n",
    "      - If the underlying data infrastructure does not allow implementing time-travel functionality, we may need to work around this in the short term by:\n",
    "        - Making sure the data science team has the right time-series expertise in order to deal with these complications manually; and\n",
    "        - We need to communicate to the business side what limitations we incur until our underlying data infrastructure supports time-travel:\n",
    "          - *Which kinds of questions we won't be able to answer,* except at great cost (e.g., \"Can you show me what the model would have predicted last January when we had this and that problem going on...\")\n",
    "          - Greater risk of training-serving skew;\n",
    "          - Slower pace of feature delivery;\n",
    "          - Greater cost: Need for more advanced skills.\n",
    "\n",
    "- Team processes:\n",
    "  - Coding standards:\n",
    "    - Python\n",
    "    - Notebook/Data-science-specific:\n",
    "      - Clean up notebook before merging.\n",
    "        - Remove duplication, e.g. resulting from copy-pasting cells and running similar training with different parameters. If we want to store the results from different experiments, this should be handled by experiment management solution such as MLFlow.\n",
    "        - Decide what we want to keep and what can be deleted. If in doubt, delete it. While it is always tempting to keep code around \"just in case\", this increases the cognitive load on the future reader and decreases the information density.\n",
    "      - Notebook output cells should reflect a run from top to bottom. \n",
    "        - This is not easy if notebook includes expensive calculations, which makes it impractical to simply restart kernel and then re-run whole notebook before committing. \n",
    "        - There are some tools that are supposed to help others, but I have not looked into them.\n",
    "        - [The best solution may be to simply limit the use of notebooks beyond initial experimentation phase](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68282.html).\n",
    "\n",
    "  - DevOps:\n",
    "    - Code versioning:\n",
    "      - Use GitHub, etc. for sharing code – don't share notebooks through email/chat, shared drive, etc.\n",
    "      - However, Continuous Integration (i.e., merging code to mainline at least once a day) is NOT generally a good fit for the iterative data science workflow. This is because notebooks require a good amount of cleanup before it makes sense to merge (see \"team processes/coding standards\" above). \n",
    "    - Environment and dependency management: Environment should be *easily reproducible* for others. See more detailed notes [here](../coding/python/readme.md).\n",
    "      - Package manager: Use pip over conda, if at all possible\n",
    "      - For short-lived notebooks, it is usually sufficient to track *either* abstract or concrete dependencies. \n",
    "      - However, if dependency conflicts become common, it is time to track both (e.g., using pipenv).\n",
    "      - It should be clear which (minor) version of python  to use to re-create an environment. Unfortunately, there is no way to encode the required python version in a requirements.txt file. So the main choices are:\n",
    "        - If we want to stick with pip-installing a requirements.txt, the best we can do is probably to define all environment-related commands in a Makefile that explicitly hard-codes the minor version of Python.\n",
    "        - Alternatively, this challenge alone can be a good reason to already learn how to use more sophisticated tools such as pipenv instead (which also bring additional benefits).\n",
    "      - If run in a managed notebook environment, it should be clear which instance size and kernel is required to run the notebook.\n",
    "      - Make sure to use reasonably up-to-date package versions. E.g., don't reuse a pre-existing environment for new project out of laziness; periodically update dependencies, etc.\n",
    "\n",
    "  - *Common strategy* for how to *version* ML-specific artifacts:\n",
    "    - models/experiments\n",
    "      - Purpose: Avoid being overwhelmed by the great number of experiments\n",
    "      - Goals:\n",
    "        - Reproducibility -> Needs to log all relevant parameters (including data versions)\n",
    "        - Low overhead -> Ideally, all experiments are automatically logged, without requiring custom code, etc.: \n",
    "    - data\n",
    "  - Align with ML engineers on choices that have a downstream impact (deployability, maintainability, etc), to avoid the risk of going down an inferior path.\n",
    "    - Double-check that environment creation process is sufficiently reproducible for production deployment.\n",
    "    - For important packages, align on which versions should be used.\n",
    "      - e.g., don't use any version of Python that is at or near its end of life. Usually, a \"middle-aged\" minor version of Python is the best choice, because it often takes surprisingly long until the newest version is supported everywhere.\n",
    "      - e.g., for Pandas, we may want to always use 2.x in order to get additional functionality and performance benefits (Arrow, copy-on-write, missing-value handling, etc).\n",
    "    - Before creating new features or training a new model, align on how the different options of doing so affect productionalizing it.\n",
    "      - This is especially important if re-training would be prohibitively expensive.\n",
    "      - At the very latest, this should be done before producing anything (code, data, models) that may be used in production. For example, it's ok to run experiments on a small subset of the data, where the main purpose is to get the code to work, but where any parameter estimates will be discarded.\n",
    "      - However, the smartest point to have this discussion is usually earlier than that, namely *before investing a considerable amount of time* into trying out something new.\n",
    "      - E.g., if Sagemaker Pipelines is used for deployment, it is easiest and safest to already use a Sagemaker Processor for preprocessing and feature engineering, and to use [an estimator from Sagemaker](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html) for model training. \n",
    "\n",
    "- individual\n",
    "  - Follow a iterative and interactive workflow\n",
    "  - ...but *clean up* code before handing off to others (whether to fellow data scientists or ML engineers) - see above.\n",
    "  - Learn to *use* the essential tooling discussed above\n",
    "\n",
    "#### Should-haves\n",
    "\n",
    "- Clean code\n",
    "- Invest time to find good data-science tools for job\n",
    "- Use engineering tools determined to be worth the investment\n",
    "  - engineering team should help with tool evaluation, recommendation, and set up\n",
    "  - Examples: Use type hints, depending on maturity\n",
    "  - leverage the power of a proper IDE (rather than notebook in browser)\n",
    "\n",
    "#### Would-like-to-haves\n",
    "\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "- Trusted test suite (automated unit and integration/acceptance tests); automated data validation, static analysis\n",
    "\n",
    "#### Does-not-need-to-haves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *ML Engineering* best practices\n",
    "\n",
    "### Must have\n",
    "\n",
    "- Clean code\n",
    "  - Rationale:\n",
    "    - Code is read much more of than written -> it should be optimized for readability.\n",
    "    - Reduces bugs.\n",
    "    - Increasing Agility, because it makes code easier to change.\n",
    "    - Overall, reduces maintenance cost (which is majority of the cost of a typical software project)\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "  - Rationale:\n",
    "    - Greatly reduces complexity, thereby ensuring code stays maintainable (reduces cost and risk, while increasing speed of feature implementation)\n",
    "\n",
    "- Trusted, automated test suite\n",
    "  - Components:\n",
    "    - unit tests\n",
    "    - integration/acceptance tests\n",
    "    - data validation\n",
    "    - static analysis\n",
    "  - Rationale:\n",
    "    - Increases quality (reduces errors, outages, etc.)\n",
    "    - Decreases costs, since the cost of bugs rises the later in the SDLC they are discovered (\"shifting left\")\n",
    "    - Indirect benefit: A trusted test suite makes sure that engineers are not dominated by their own creation\n",
    "- Type safety:\n",
    "  - code: use type hints, and force in CI pipeline\n",
    "  - data: use explicit data schemas\n",
    "- Observability:\n",
    "  - code\n",
    "    - structure & centralized logging\n",
    "    - monitoring and alerting\n",
    "    - distributed tracing if using micro service architecture\n",
    "  - model performance\n",
    "    - comparison between different models\n",
    "    - comparison of same model over time (model drift?)\n",
    "    - performance for different subsets of the population/bias (if substantial, does this vary by model?).\n",
    "  - data\n",
    "    - data quality\n",
    "- DevOps\n",
    "  - Use infrastructure-as-code\n",
    "    - CI/CD\n",
    "    - Enforce quality gates in pipeline\n",
    "      - in particular: \n",
    "        - tests: run unit and acceptance tests, check test coverage threshold, static code analysis (especially type checking)\n",
    "        - readability: linting or auto-formatting, code complexity checks\n",
    "        - security scans: scan dependencies for vulnerabilities and license risk scan, static analysis\n",
    "      - Note: Enforcement requires that pipeline blocks deployment if any of these checks fail.\n",
    "- Data lineage\n",
    "- Invest time to find good tools for the job\n",
    "  - Avoid reinventing the wheel -> Leverage managed services wherever possible (unless “unfair” pricing)\n",
    "  - leverage power of IDE (rather than notebook in browser)\n",
    "\n",
    "### Should have\n",
    "- DevOps:\n",
    "  - Infra-as-Code:\n",
    "    - Also manage most of the *data science* infrastructure using IaC. \n",
    "      - The reason I say \"most\" is because we get the biggest bang for our buck by focusing on the constant/long-lived infrastructure components; if a data scientist wants to try out some new resources, it's fine to create it using the console - thereby, we avoid introducing dependencies/blockers.\n",
    "  - CI/CD:\n",
    "      - Manual modifications to prod should only be possible in emergencies \n",
    "        - Engineers shoul have read-only access to prod\n",
    "        - \"break-glass account\" (with sign-off process) for emergencies\n",
    "- Profile code and optimize performance bottlenecks\n",
    "  - (Why we do not consider this a must-have: Engineers' time is very expensive, and so is delaying features or accumulating technical debt, so unfortunately performance optimization has sometimes to be sacrificed for these even more important goals.\n",
    "- Periodically reevaluate if there are better tools for the job\n",
    "  - e.g., Pandas alternatives (such as Polars)\n",
    "  - e.g., Spark vs Presto\n",
    "  - e.g., End-to-end (Sagemaker ) versus best-off-breed MLOps tools\n",
    "\n",
    "### Would-like to have\n",
    "\n",
    "### (Does not need to have:)\n",
    "\n",
    "- Best practices for data science:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presentations-X1dq4RR9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
