{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Shared best practices~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "-   Open questions\n",
    "    - Is organizing by must have versus should have the best idea?\n",
    "    - Where do we draw the line between must versus should have? \n",
    "    - How do we deal with differences in degree rather than differences in kind? For example, a basic amount of clean code is clearly a must-have for ML engineering, but some aspects of it may only be should- or even would-like-to haves.\n",
    "\n",
    "-  Alternatives:\n",
    "    - ~~by maturity level~~ \n",
    "      - Corresponds to must/should/would like to haves: Low maturity corresponds to not having any; medium maturity corresponds to having must-haves, high corresponds to additionally satisfying should-haves, and highest corresponds to also satisfying would-like-to-haves.\n",
    "    - Should we specify which are the minimum requirements once we enter production? Or could just use color-coding!\n",
    "\n",
    "-  Note that these may also differ by:\n",
    "    - use case (e.g., inherent complexity of data transformations and ML modeling)\n",
    "    - POC versus production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data science* best practices\n",
    "## Must-haves\n",
    "### Background infra / team investment ~~/ Environment / Ecosystem~~\n",
    "  - Quick & easy dev setup\n",
    "    - Align incentives: The recommended way should be the easy way. \n",
    "      - Don't just write up the required setup steps in a readme – instead, put them in a script whenever possible! [Makefiles are ideals for this](./_details/why_makefiles.md).\n",
    "  - Availability of tooling to:\n",
    "    - manage the complexity that arises out of the *explorative* workflow \n",
    "      - In particular, a solution for experiment management (e.g., MLFlow). A model registry also falls into this category, as it serves to mark which of the many trained models have passed a quality threshold that makes them fit for deployment.\n",
    "    - deal with *data versioning*\n",
    "      - For basic use cases, Data Version Control may be a good enough starting point.\n",
    "      - If the data has an important time-series component, the ability to time-travel (e.g., retrieve a snapshot of how the data looked like at a given point in time) becomes essential in order to avoid information leakage, which is a common source of training-serving skew.\n",
    "        - This functionality could either come from either:\n",
    "          - the underlying data infrastructure (e.g. it is available in Snowflake, DeltaLake, Apache Iceberg tables, or any system based on event-sourcing); or\n",
    "          - a dedicated feature store.\n",
    "      - If the underlying data infrastructure does not allow implementing time-travel functionality, we may need to work around this in the short term by:\n",
    "        - Making sure the data science team has the right time-series expertise in order to deal with these complications manually; and\n",
    "        - We need to communicate to the business side what limitations we incur until our underlying data infrastructure supports time-travel:\n",
    "          - *Which kinds of questions we won't be able to answer,* except at great cost (e.g., \"Can you show me what the model would have predicted last January when we had this and that problem going on...\")\n",
    "          - Greater risk of training-serving skew;\n",
    "          - Slower pace of feature delivery;\n",
    "          - Greater cost: Need for more advanced skills.\n",
    "  - Safety to experiment\n",
    "    - This is analogous to the importance of psychological safety at the team level.\n",
    "\n",
    "### Team processes:\n",
    "  - Coding standards:\n",
    "    - Python:\n",
    "      - auto-format code (or follow at least basic coding standards).\n",
    "        - At the latest, auto-formatting should be run by pipeline on merge.\n",
    "        - It may also make sense to have the IDE auto-format files on save, because this instant feedback is a great way to teach data scientists how to write better code in the first place. However, it is crucial that IDE and pipeline apply the *same* rules (e.g., are configured from the same .pylintrc file).\n",
    "        - Rationale: Ensure  consistency; *enforce* best practices, such as optimizing for readability and avoiding surprising or error-prone constructs. A lot has already been written about this topic; see for example [the excellent discussion in \"Software Engineering at Google\"](https://abseil.io/resources/swe-book/html/ch08.html).\n",
    "        - Data-science specific modifications:\n",
    "          - Since data scientists tend to be less interested in learning the details of a programming language, we need to find a process that does not require them to spend much time on mastering the rules. Therefore, the *easiest solution is usually to simply set up automatic code formatting* (e.g., using auto-pep8, black, yapf, etc). This reduces the effort to a one-time set up, and can be handled by the engineering team.\n",
    "\n",
    "    - Notebook/Data-science-specific:\n",
    "      - Clean up notebook before merging: It is natural that notebooks get messy from the explorative data science workflow. This is not a problem in itself, but the key is to eventually distill the essential insights into a more easily digestible form. The right time to do so is when merging to mainline, because this is when the work will be shared with other people, who need to be able to understand it with the least possible cognitive overhead.\n",
    "        - Remove duplication, e.g. resulting from copy-pasting cells and running similar transformations/training with different parameters. If we want to store the results from different experiments, this should be handled by experiment management solution such as MLFlow.\n",
    "        - Decide what we want to keep around, and what can be deleted. (If in doubt, it is probably fine to delete. While it is always tempting to keep code around \"just in case\", this increases the cognitive load on the future reader and detracts from the main insights.)\n",
    "      - Ensure notebook output cells reflect a run from top to bottom. \n",
    "        - This is not easy if notebook includes expensive calculations, which makes it impractical to simply restart kernel and then re-run whole notebook before committing. \n",
    "        - There are some tools that are supposed to help with this, but I have not looked into them.\n",
    "        - [The best solution may be to simply limit the use of notebooks beyond the initial experimentation phase](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68282.html).\n",
    "      - Make the *intent* clear, so code makes sense to others\n",
    "        - Basic refactoring:\n",
    "          - Improve variable naming\n",
    "          - Remove any hacks\n",
    "        - Add comments - in particular about information that can not be gathered from the code itself, such as the \"why?\".\n",
    "\n",
    "  - DevOps:\n",
    "    - Code versioning:\n",
    "      - Use GitHub, etc. for sharing code – don't share notebooks through email/chat, shared drive, etc.\n",
    "      - However, Continuous Integration (i.e., merging code to mainline at least once a day) is NOT generally a good fit for the explorative data science workflow. This is because notebooks require a good amount of cleanup before it makes sense to merge (see \"team processes/coding standards\" above). \n",
    "    - Environment and dependency management: Environment should be *easily reproducible* for others. See more detailed notes [here](../coding/python/readme.md).\n",
    "      - Package manager: Use pip over conda, if at all possible\n",
    "      - For short-lived notebooks, it is usually sufficient to track *either* abstract or concrete dependencies. \n",
    "      - However, if dependency conflicts become common, it is time to track both (e.g., using pipenv).\n",
    "      - It should be clear which (minor) version of python  to use to re-create an environment. Unfortunately, there is no way to encode the required python version in a requirements.txt file. So the main choices are:\n",
    "        - If we want to stick with pip-installing a requirements.txt, the best we can do is probably to define all environment-related commands in a Makefile that explicitly hard-codes the minor version of Python.\n",
    "        - Alternatively, this challenge alone can be a good reason to already learn how to use more sophisticated tools such as pipenv instead (which also bring additional benefits).\n",
    "      - If run in a managed notebook environment, it should be clear which instance size and kernel is required to run the notebook.\n",
    "      - Make sure to use reasonably up-to-date package versions. E.g., don't reuse a pre-existing environment for new project out of laziness; periodically update dependencies, etc.\n",
    "\n",
    "  - *Common strategy* for how to *version* ML-specific artifacts:\n",
    "    - models/experiments\n",
    "      - Purpose: Avoid being overwhelmed by the great number of experiments\n",
    "      - Goals:\n",
    "        - Reproducibility -> Needs to log all relevant parameters (including data versions)\n",
    "        - Low overhead -> Ideally, all experiments are automatically logged, without requiring custom code, etc.: \n",
    "    - data\n",
    "  - Align with ML engineers on choices that have a downstream impact (deployability, maintainability, etc), to avoid the risk of going down an inferior path.\n",
    "    - Double-check that environment creation process is sufficiently reproducible for production deployment.\n",
    "    - For important packages, align on which versions should be used.\n",
    "      - e.g., don't use any version of Python that is at or near its end of life. Usually, a \"middle-aged\" minor version of Python is the best choice, because it often takes surprisingly long until the newest version is supported everywhere.\n",
    "      - e.g., for Pandas, we may want to always use 2.x in order to get additional functionality and performance benefits (Arrow, copy-on-write, missing-value handling, etc).\n",
    "    - Before creating new features or training a new model, align on how the different options of doing so affect productionalizing it.\n",
    "      - This is especially important if re-training would be prohibitively expensive.\n",
    "      - At the very latest, this should be done before producing anything (code, data, models) that may be used in production. For example, it's ok to run experiments on a small subset of the data, where the main purpose is to get the code to work, but where any parameter estimates will be discarded.\n",
    "      - However, the smartest point to have this discussion is usually earlier than that, namely *before investing a considerable amount of time* into trying out something new.\n",
    "      - E.g., if Sagemaker Pipelines is used for deployment, it is easiest and safest to already use a Sagemaker Processor for preprocessing and feature engineering, and to use [an estimator from Sagemaker](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html) for model training. \n",
    "\n",
    "### individual\n",
    "  - Follow a explorative and interactive workflow\n",
    "  - ...but *clean up* code before handing off to others (whether to fellow data scientists or ML engineers) - see above.\n",
    "  - Learn to *use* the essential tooling discussed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should-haves\n",
    "\n",
    "- Clean code\n",
    "- Invest time to find good data-science tools for job\n",
    "- Use engineering tools determined to be worth the investment\n",
    "  - engineering team should help with tool evaluation, recommendation, and set up\n",
    "  - Examples: Use type hints, depending on maturity\n",
    "  - leverage the power of a proper IDE (rather than notebook in browser)\n",
    "\n",
    "## Would-like-to-haves\n",
    "\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "- Trusted test suite (automated unit and integration/acceptance tests); automated data validation, static analysis\n",
    "\n",
    "## Does-not-need-to-haves\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presentations-X1dq4RR9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
