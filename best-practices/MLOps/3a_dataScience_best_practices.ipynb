{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Shared best practices~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "-   Open questions\n",
    "    - Is organizing by must have versus should have the best idea?\n",
    "    - Where do we draw the line between must versus should have? \n",
    "    - How do we deal with differences in degree rather than differences in kind? For example, a basic amount of clean code is clearly a must-have for ML engineering, but some aspects of it may only be should- or even would-like-to haves.\n",
    "\n",
    "-  Alternatives:\n",
    "    - ~~by maturity level~~ \n",
    "      - Corresponds to must/should/would like to haves: Low maturity corresponds to not having any; medium maturity corresponds to having must-haves, high corresponds to additionally satisfying should-haves, and highest corresponds to also satisfying would-like-to-haves.\n",
    "    - Should we specify which are the minimum requirements once we enter production? Or could just use color-coding!\n",
    "\n",
    "-  Note that these may also differ by:\n",
    "    - use case (e.g., inherent complexity of data transformations and ML modeling)\n",
    "    - POC versus production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Goals (data science)\n",
    "- Reproducible analysis\n",
    "  - Code\n",
    "  - Environment\n",
    "  - Data\n",
    "  - Results\n",
    "- Efficient collaboration with and handover to ML engineers (since the end goal is deployment to production)\n",
    "- Easy setup: Minimize *time* scientists spend with DevOps/software dev work (e.g., environment setup, etc)\n",
    "- Minimize *knowledge* requirement of software dev tools and concepts\n",
    "  - Note that this goal often conflicts with reproducible analysis and efficient handover to engineering, requiring difficult trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data science* best practices\n",
    "## Must-haves\n",
    "### 1) Background infra / team investment ~~/ Environment / Ecosystem~~\n",
    "- Quick & easy dev setup\n",
    "  - Align incentives: The recommended way should be the easy way. \n",
    "    - Don't just write up the required setup steps in a readme â€“ instead, put them in a script whenever possible! [Makefiles are ideals for this](./_details/why_makefiles.md).\n",
    "  - This is most efficiently solved by a central (company-wide) ML infrastructure team, so that each team does not have to reinvent the wheel. However, were such a central team does not yet exist, this task can easily be handled by ML engineers. (In the latter case, this is a good candidate for scaling the solution develop to other teams, once the benefits can be demonstrated.)\n",
    "  - Try to leverage *managed* services, rather than re-inventing the wheel! Because undifferentiated heavy lifting is more cheaply carried out by specialized providers, it pays to focus on a company's *core competencies*. Most companies tend to under-rely on managed services because:\n",
    "    - They systematically under-estimate the relative cost of building versus buying, because they:\n",
    "      - are overly focused on the additional out-of-pocket *expenses* of managed services, while not sufficiently taking into account the value of engineers' time that this frees up;\n",
    "      - don't sufficiently account for the risk of \"unknown unknowns\" of implementing a solution themselves ([\"planning fallacy\"](https://en.wikipedia.org/wiki/Planning_fallacy));\n",
    "    - If there is already an internal team currently providing these services in-house, these internal stakeholders - who now risk loosing their project - often have disproportionate influence on decision-making. This is \n",
    "      - because of [loss aversion](https://en.wikipedia.org/wiki/Loss_aversion), i.e. people tend to be more sensitive to losses compared to gains;\n",
    "      - because the losses are *concentrated* while the gains are *spread out*, making it easier for the would-be losers to organize and lobby for their interests.\n",
    "- Availability of tooling to:\n",
    "  - Use the explorative notebook workflow without forgoing the benefits of an IDE (e.g., syntax highlighting, code completion, easy display of documentation such as parameter names of a function, ability to use powerful plugins such as GitHub Co-Pilot, etc)\n",
    "    - Reliable solution to run Notebook in IDE (VSCode support for notebooks is borderline as of late 2023 - net positive in my opinion but still a lot of bugs, though no dealbreakers)\n",
    "    - Promising alternative: JetBrains Datalore. Managed notebook environment with a lot of additional features (in particular, inbuilt visualization of data frames, similar to what DataBricks does for Spark; feature for collaboration and sharing of interactive reports). Need to evaluate in more detail if it is worth the cost, but if it has less bugs than VSCode, the time it saves data scientists should easily outweigh the subscription cost.\n",
    "  - manage the complexity that arises out of the *explorative* workflow \n",
    "    - In particular, a solution for experiment management (e.g., MLFlow). A model registry also falls into this category, as it serves to mark which of the many trained models have passed a quality threshold that makes them fit for deployment.\n",
    "  - deal with *data versioning*\n",
    "    - For basic use cases, Data Version Control may be a good enough starting point.\n",
    "    - If the data has an important time-series component, the ability to time-travel (e.g., retrieve a snapshot of how the data looked like at a given point in time) becomes essential in order to avoid information leakage, which is a common source of training-serving skew.\n",
    "      - This functionality could come from either:\n",
    "        - the underlying data infrastructure (e.g. it is available in Snowflake, DeltaLake, Apache Iceberg tables, or any system based on event-sourcing); or\n",
    "        - a dedicated feature store.\n",
    "    - If the underlying data infrastructure does not allow implementing time-travel functionality, we may need to work around this in the short term by:\n",
    "      - Making sure the data science team has the right time-series expertise in order to deal with these complications manually; and\n",
    "      - We need to communicate to the business side what limitations we incur until our underlying data infrastructure supports time-travel:\n",
    "        - *Which kinds of questions we won't be able to answer,* except at great cost (e.g., \"Can you show me what the model would have predicted last January when we had this and that problem going on...\")\n",
    "        - Greater risk of training-serving skew;\n",
    "        - Slower pace of feature delivery;\n",
    "        - Greater cost: Need for more advanced skills.\n",
    "  - Share productionalized features between teams to:\n",
    "    - Avoid duplicating work. (Reducing this waste is even more important since data cleaning and feature engineering tend to be among the less favorite part of a data scientists job. Thus, by allowing them to spend a greater proportion of their time on actually building models, we increase job satisfaction of data scientists, and thus likely improve retention.)\n",
    "    - Ensure different teams use same definition of how exactly to *operationalize* important business concepts. This:\n",
    "      - limits differnt teams and organizational units from drifting apart and getting increasingly siloed;\n",
    "      - makes it easier to compare the performance of different models. (If we calculate features in different ways, we introduce yet another variable that could explain different results.)\n",
    "\n",
    "- Safety to experiment: \n",
    "  - The easy/default way should be the safe way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Team processes:\n",
    "- Coding standards:\n",
    "  - Python:\n",
    "    - auto-format code  (but don't spend much time worrying about details)\n",
    "      - The reason I consider this a must-have - even though it may not seem highest priority - is that it is a *quick win,* delivering outsized benefits relative to the small amount of work required. Often, it will be possible to simply borrow the setup that ML engineers already have. If not, remember that any autoformatter is better than none, so just pick one and get started! \n",
    "      - At the latest, auto-formatting should be run by pipeline on merge.\n",
    "      - It may also make sense to have the IDE auto-format files on save, because this instant feedback is a great way to coach data scientists over time how to write better code in the first place. However, it is crucial that IDE and pipeline apply the *same* rules (e.g., are configured from the same .pylintrc file). \n",
    "      - Rationale: Ensure  consistency; *enforce* best practices, such as optimizing for readability and avoiding surprising or error-prone constructs. A lot has already been written about this topic; see for example [the excellent discussion in \"Software Engineering at Google\"](https://abseil.io/resources/swe-book/html/ch08.html). \n",
    "      - Data-science specific modifications:\n",
    "        - Since data scientists tend to be less interested in learning the details of a programming language, we need to find a process that does not require them to spend much time mastering the rules. Therefore, the *easiest solution is usually to simply set up automatic code formatting* (using auto-pep8, black, yapf, etc). This reduces the effort to a one-time set up, which can be handled by the engineering team.\n",
    "        - Generally, we want to align with the engineering team on using the same formatting standards (i.e., use same linting config file). If handoff from data science to engineering is based on branching (i.e., commits flow from a \"data-science\" branch to an \"ml-engineering\" branch), we could technically apply a different set of formatting rules at the point of that merge. However, unless there is a clear reason why there are different formatting needs, it is better to avoid this to not only simplify the process but also increase the consistency of the code base.\n",
    "  - Notebook/Data-science-specific:\n",
    "    - Clean up notebook before merging: It is completely natural that notebooks get messy from the explorative data science workflow. This is not a problem in itself - but the key is to eventually distill the essential insights into a more easily digestible form. The right time to do so is when merging to mainline, as this is when the work will be shared with other people, who need to be able to understand it with the least possible cognitive overhead. \n",
    "      - Extract functions into a separate python module (and import them into the notebook) if:\n",
    "        - any of the analysis is promising for reuse; or\n",
    "        - analysis will be handed off to others to refine or productionize. (In this case, the rationale is primarily readability).\n",
    "        - Rationale:\n",
    "          - Follow the DRY (Do Not Repeat Yourself) principle.\n",
    "          - Certain operations work better on python files as opposed to notebook cells (e.g., unit tests, real-time static analysis)\n",
    "        - Makes the analysis easier to understand for others, because it separates levels of abstractions (which is one of the \"clean code\" principles): The notebook contains the high-level analysis, and if you want to delve deeper into the details of what a given function is doing, you can go to the function definition in the module.\n",
    "        - In addition, separating the interactive notebook from the core functions offers a great way to use our screen real estate efficiently by opening notebook and python module side-by-side. This is in my experience the best way to solve the problem that notebooks tend to get very long due to the output, thus require a lot of scrolling and making it easy to get lost. This way, the python module containing the important details of the code does *not* suffer from the same problem.\n",
    "      - Remove duplication, e.g. resulting from copy-pasting cells and running similar transformations/training with different parameters. If we want to store the results from different experiments, this should be handled by experiment management solution such as MLFlow.\n",
    "      - Decide what we want to keep around, and what can be deleted. (If in doubt, it is probably fine to delete. While it is always tempting to keep code around \"just in case\", this increases the cognitive load on the future reader and detracts from the main insights.)\n",
    "    - Ensure notebook output cells reflect a run from top to bottom. \n",
    "      - This is not easy if notebook includes expensive calculations, which makes it impractical to simply restart kernel and then re-run whole notebook before committing. \n",
    "      - There are some tools that are supposed to help with this, but I have not looked into them.\n",
    "      - [The best solution may be to simply limit the use of notebooks beyond the initial experimentation phase](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68282.html).\n",
    "    - Make the *intent* clear, so code makes sense to others\n",
    "      - Basic refactoring:\n",
    "        - Improve variable naming\n",
    "        - Remove any hacks\n",
    "      - Add comments - in particular about information that can not be gathered from the code itself, such as the \"why?\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DevOps:\n",
    "  - Code versioning:\n",
    "    - Use GitHub, etc. for sharing code â€“ don't share notebooks through email/chat, shared drive, etc.\n",
    "    - However, Continuous Integration (i.e., merging code to mainline at least once a day) is NOT generally a good fit for the explorative data science workflow. This is because notebooks require a good amount of cleanup before it makes sense to merge (see \"team processes/coding standards\" above).\n",
    "  - Environment and dependency management: Environment should be *easily reproducible* for others. See more detailed notes [here](../coding/python/readme.md).\n",
    "    - Package manager: Use pip over conda, if at all possible\n",
    "    - For short-lived notebooks, it is usually sufficient to track *either* abstract or concrete dependencies.\n",
    "    - However, if dependency conflicts become common, it is time to track both (e.g., using pipenv).\n",
    "    - It should be clear which (minor) version of python  to use to re-create an environment. Unfortunately, there is no way to encode the required python version in a requirements.txt file. So the main choices are:\n",
    "      - If we want to stick with pip-installing a requirements.txt, the best we can do is probably to define all environment-related commands in a Makefile that explicitly hard-codes the minor version of Python.\n",
    "      - Alternatively, this challenge alone can be a good reason to already learn how to use more sophisticated tools such as pipenv instead (which also bring additional benefits).\n",
    "    - If run in a managed notebook environment, it should be clear which instance size and kernel is required to run the notebook.\n",
    "    - Make sure to use reasonably up-to-date package versions. E.g., don't reuse a pre-existing environment for new project out of laziness; periodically update dependencies, etc.\n",
    "\n",
    "- *Common strategy* for how to *version* ML-specific artifacts:\n",
    "  - models/experiments\n",
    "    - Purpose: Avoid being overwhelmed by the great number of experiments\n",
    "    - Goals:\n",
    "      - Reproducibility -> Needs to log all relevant parameters (including data versions)\n",
    "      - Low overhead -> Ideally, all experiments are automatically logged, without requiring custom code, etc.:\n",
    "  - data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Align with ML engineers on choices that have a downstream impact (deployability, maintainability, etc), to avoid the risk of going down an inferior path.\n",
    "    - Double-check that environment creation process is sufficiently reproducible for production deployment.\n",
    "    - For important packages, align on which versions should be used.\n",
    "      - e.g., don't use any version of Python that is at or near its end of life. Usually, a \"middle-aged\" minor version of Python is the best choice, because it often takes surprisingly long until the newest version is supported everywhere.\n",
    "      - e.g., for Pandas, we may want to always use 2.x in order to get additional functionality and performance benefits (Arrow, copy-on-write, missing-value handling, etc).\n",
    "    - Before creating new features or training a new model, align on how the different options of doing so affect productionalizing it.\n",
    "      - This is especially important if re-training would be prohibitively expensive.\n",
    "      - At the very latest, this should be done before producing anything (code, data, models) that may be used in production. For example, it's ok to run experiments on a small subset of the data, where the main purpose is to get the code to work, but where any parameter estimates will be discarded.\n",
    "      - However, the smartest point to have this discussion is usually earlier than that, namely *before investing a considerable amount of time* into trying out something new.\n",
    "      - E.g., if Sagemaker Pipelines is used for deployment, it is easiest and safest to already use a Sagemaker Processor for preprocessing and feature engineering, and to use [an estimator from Sagemaker](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html) for model training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) individual\n",
    "  - Follow a explorative and interactive workflow\n",
    "  - ...but *clean up* code before handing off to others (whether to fellow data scientists or ML engineers) - see above.\n",
    "  - Learn to *use* the essential tooling discussed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should-haves\n",
    "\n",
    "- Clean code\n",
    "  - Coding standards: potentially customize configuration of linting, etc. to the team's specific needs. Remember that the rationale for considering automatic code formatting a must-have is that it is a quick win to set it up with default parameters - that's why it is important not to spend too much time tweaking the linting parameters until we have reached this should-have stage. At this point, we may also consider switching to a different linting tool (e.g., move from Black to Pylint for more customizability).\n",
    "- Invest time to find good data-science tools for job\n",
    "- Use engineering tools determined to be worth the investment\n",
    "  - engineering team should help with tool evaluation, recommendation, and set up\n",
    "  - Examples: Use type hints, depending on maturity\n",
    "  - leverage the power of a proper IDE (rather than notebook in browser)\n",
    "\n",
    "## Would-like-to-haves\n",
    "\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "- Trusted test suite (automated unit and integration/acceptance tests); automated data validation, static analysis\n",
    "\n",
    "## Does-not-need-to-haves\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presentations-X1dq4RR9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
