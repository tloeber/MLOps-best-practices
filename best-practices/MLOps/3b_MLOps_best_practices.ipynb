{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ML Engineering* best practices\n",
    "## Must have\n",
    "\n",
    "- Clean code\n",
    "  - Rationale:\n",
    "    - Code is read much more of than written -> it should be optimized for readability.\n",
    "    - Reduces bugs.\n",
    "    - Increasing Agility, because it makes code easier to change.\n",
    "    - Overall, reduces maintenance cost (which is majority of the cost of a typical software project)\n",
    "- Leverage design patterns to achieve loose coupling between components\n",
    "  - Rationale:\n",
    "    - Greatly reduces complexity, thereby ensuring code stays maintainable (reduces cost and risk, while increasing speed of feature implementation)\n",
    "\n",
    "- Trusted, automated test suite\n",
    "  - Components:\n",
    "    - unit tests\n",
    "    - integration/acceptance tests\n",
    "    - data validation\n",
    "    - static analysis\n",
    "  - Rationale:\n",
    "    - Increases quality (reduces errors, outages, etc.)\n",
    "    - Decreases costs, since the cost of bugs rises the later in the SDLC they are discovered (\"shifting left\")\n",
    "    - Indirect benefit: A trusted test suite makes sure that engineers are not dominated by their own creation\n",
    "- Type safety:\n",
    "  - code: use type hints, and force in CI pipeline\n",
    "  - data: use explicit data schemas\n",
    "- Observability:\n",
    "  - code\n",
    "    - structure & centralized logging\n",
    "    - monitoring and alerting\n",
    "    - distributed tracing if using micro service architecture\n",
    "  - model performance\n",
    "    - comparison between different models\n",
    "    - comparison of same model over time (model drift?)\n",
    "    - performance for different subsets of the population/bias (if substantial, does this vary by model?).\n",
    "  - data\n",
    "    - data quality\n",
    "- DevOps\n",
    "  - Use infrastructure-as-code\n",
    "    - CI/CD\n",
    "    - Enforce quality gates in pipeline\n",
    "      - in particular: \n",
    "        - tests: run unit and acceptance tests, check test coverage threshold, static code analysis (especially type checking)\n",
    "        - readability: linting or auto-formatting, code complexity checks\n",
    "        - security scans: scan dependencies for vulnerabilities and license risk scan, static analysis\n",
    "      - Note: Enforcement requires that pipeline blocks deployment if any of these checks fail.\n",
    "- Data lineage\n",
    "- Invest time to find good tools for the job\n",
    "  - Avoid reinventing the wheel -> Leverage managed services wherever possible (unless “unfair” pricing)\n",
    "  - leverage power of IDE (rather than notebook in browser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should have\n",
    "- DevOps:\n",
    "  - Infra-as-Code:\n",
    "    - Also manage most of the *data science* infrastructure using IaC. \n",
    "      - The reason I say \"most\" is because we get the biggest bang for our buck by focusing on the constant/long-lived infrastructure components; if a data scientist wants to try out some new resources, it's fine to create it using the console - thereby, we avoid introducing dependencies/blockers.\n",
    "  - CI/CD:\n",
    "      - Manual modifications to prod should only be possible in emergencies \n",
    "        - Engineers shoul have read-only access to prod\n",
    "        - \"break-glass account\" (with sign-off process) for emergencies\n",
    "- Profile code and optimize performance bottlenecks\n",
    "  - (Why we do not consider this a must-have: Engineers' time is very expensive, and so is delaying features or accumulating technical debt, so unfortunately performance optimization has sometimes to be sacrificed for these even more important goals.\n",
    "- Periodically reevaluate if there are better tools for the job\n",
    "  - e.g., Pandas alternatives (such as Polars)\n",
    "  - e.g., Spark vs Presto\n",
    "  - e.g., End-to-end (Sagemaker ) versus best-off-breed MLOps tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Would-like to have\n",
    "\n",
    "## (Does not need to have:)\n",
    "\n",
    "- Best practices for data science:\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
